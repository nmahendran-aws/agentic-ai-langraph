{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "debug_info",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Python Executable: /Users/mahen/learnings/AI/ai-agent/agentic-ai/langchain/tools/.venv/bin/python\n",
                        "Current Working Directory: /Users/mahen/learnings/AI/ai-agent/agentic-ai/langchain/tools\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ langchain_openai is installed!\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import os\n",
                "print(f\"Python Executable: {sys.executable}\")\n",
                "print(f\"Current Working Directory: {os.getcwd()}\")\n",
                "try:\n",
                "    import langchain_openai\n",
                "    print(\"‚úÖ langchain_openai is installed!\")\n",
                "except ImportError:\n",
                "    print(\"‚ùå langchain_openai is NOT installed in this environment.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a1b2c3d4",
            "metadata": {},
            "source": [
                "# LangGraph + GPT-4 Hello World\n",
                "\n",
                "This notebook demonstrates how to build a simple conversational agent using LangGraph and OpenAI's GPT-4."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "imports",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from typing import TypedDict, Annotated\n",
                "from dotenv import load_dotenv\n",
                "from langchain_openai import ChatOpenAI\n",
                "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage\n",
                "from langgraph.graph import StateGraph, add_messages\n",
                "\n",
                "# Load API keys from .env file\n",
                "load_dotenv()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "state_definition",
            "metadata": {},
            "source": [
                "## Define the Agent State\n",
                "\n",
                "The state uses `Annotated[list[BaseMessage], add_messages]` which means:\n",
                "- Type: A list of messages\n",
                "- Reducer: `add_messages` appends new messages instead of replacing them"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "state",
            "metadata": {},
            "outputs": [],
            "source": [
                "class AgentState(TypedDict):\n",
                "    \"\"\"State schema for our conversational agent.\"\"\"\n",
                "    messages: Annotated[list[BaseMessage], add_messages]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "model_function",
            "metadata": {},
            "source": [
                "## Define the Model Calling Function\n",
                "\n",
                "This function:\n",
                "1. Initializes GPT-4\n",
                "2. Adds a system message for context\n",
                "3. Invokes GPT-4 with the conversation history\n",
                "4. Returns the response"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "call_model",
            "metadata": {},
            "outputs": [],
            "source": [
                "def call_model(state: AgentState) -> dict:\n",
                "    \"\"\"Invoke GPT-4 with the conversation history.\"\"\"\n",
                "    \n",
                "    # Initialize GPT-4\n",
                "    llm = ChatOpenAI(\n",
                "        model=\"gpt-4\",           # Use GPT-4 model\n",
                "        temperature=0.7,         # Controls randomness (0 = deterministic, 2 = very creative)\n",
                "        max_tokens=500           # Limit response length\n",
                "    )\n",
                "    \n",
                "    # Add a system message to set the assistant's behavior\n",
                "    system_msg = SystemMessage(\n",
                "        content=\"You are a helpful and friendly AI assistant. Be concise but informative.\"\n",
                "    )\n",
                "    \n",
                "    # Combine system message with conversation history\n",
                "    messages = [system_msg] + state[\"messages\"]\n",
                "    \n",
                "    # Invoke GPT-4 and get the response\n",
                "    response = llm.invoke(messages)\n",
                "    \n",
                "    # Return the response (it will be appended to messages)\n",
                "    return {\"messages\": [response]}"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "graph_setup",
            "metadata": {},
            "source": [
                "## Build the LangGraph\n",
                "\n",
                "Create a simple graph with one node that calls GPT-4."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "graph",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Graph compiled successfully!\n"
                    ]
                }
            ],
            "source": [
                "# Create the graph\n",
                "graph = StateGraph(AgentState)\n",
                "\n",
                "# Add the model calling node\n",
                "graph.add_node(\"assistant\", call_model)\n",
                "\n",
                "# Set entry and finish points\n",
                "graph.set_entry_point(\"assistant\")\n",
                "graph.set_finish_point(\"assistant\")\n",
                "\n",
                "# Compile the graph\n",
                "app = graph.compile()\n",
                "\n",
                "print(\"‚úÖ Graph compiled successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "test1",
            "metadata": {},
            "source": [
                "## Test 1: Simple Question"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "example1",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== Conversation ===\n",
                        "\n",
                        "HUMAN: What is LangGraph?\n",
                        "\n",
                        "AI: LangGraph is not a universally recognized term or tool in technology or linguistics. It might be a specific tool or concept within a certain context, organization, or software. However, without more specific information, it's challenging to provide a detailed explanation. Please provide more context.\n"
                    ]
                }
            ],
            "source": [
                "# Test with a simple question\n",
                "result = app.invoke({\n",
                "    \"messages\": [HumanMessage(content=\"What is LangGraph?\")]\n",
                "})\n",
                "\n",
                "# Print the conversation\n",
                "print(\"=== Conversation ===\")\n",
                "for msg in result[\"messages\"]:\n",
                "    print(f\"\\n{msg.type.upper()}: {msg.content}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "test2",
            "metadata": {},
            "source": [
                "## Test 2: Multi-turn Conversation\n",
                "\n",
                "Demonstrate how messages accumulate in the state."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "example2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== Turn 1 ===\n",
                        "Assistant: Hello, Mahendran! How can I assist you today?\n",
                        "\n",
                        "=== Turn 2 ===\n",
                        "Assistant: Your name is Mahendran.\n",
                        "\n",
                        "=== Full Conversation ===\n",
                        "\n",
                        "HUMAN: Hi! My name is Mahendran.\n",
                        "\n",
                        "AI: Hello, Mahendran! How can I assist you today?\n",
                        "\n",
                        "HUMAN: What's my name?\n",
                        "\n",
                        "AI: Your name is Mahendran.\n"
                    ]
                }
            ],
            "source": [
                "# Start a multi-turn conversation\n",
                "conversation = {\n",
                "    \"messages\": [\n",
                "        HumanMessage(content=\"Hi! My name is Mahendran.\"),\n",
                "    ]\n",
                "}\n",
                "\n",
                "# First turn\n",
                "result1 = app.invoke(conversation)\n",
                "print(\"=== Turn 1 ===\")\n",
                "print(f\"Assistant: {result1['messages'][-1].content}\")\n",
                "\n",
                "# Second turn - the state now contains previous messages\n",
                "result1[\"messages\"].append(HumanMessage(content=\"What's my name?\"))\n",
                "result2 = app.invoke(result1)\n",
                "print(\"\\n=== Turn 2 ===\")\n",
                "print(f\"Assistant: {result2['messages'][-1].content}\")\n",
                "\n",
                "# Print full conversation\n",
                "print(\"\\n=== Full Conversation ===\")\n",
                "for msg in result2[\"messages\"]:\n",
                "    if msg.type in [\"human\", \"ai\"]:\n",
                "        print(f\"\\n{msg.type.upper()}: {msg.content}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "test3",
            "metadata": {},
            "source": [
                "## Test 3: Different GPT-4 Models\n",
                "\n",
                "Try different OpenAI models to see the difference."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "example3",
            "metadata": {},
            "outputs": [],
            "source": [
                "def test_model(model_name: str, question: str):\n",
                "    \"\"\"Test a specific OpenAI model.\"\"\"\n",
                "    \n",
                "    def call_specific_model(state: AgentState) -> dict:\n",
                "        llm = ChatOpenAI(model=model_name, temperature=0)\n",
                "        system_msg = SystemMessage(content=\"You are a helpful assistant.\")\n",
                "        messages = [system_msg] + state[\"messages\"]\n",
                "        response = llm.invoke(messages)\n",
                "        return {\"messages\": [response]}\n",
                "    \n",
                "    g = StateGraph(AgentState)\n",
                "    g.add_node(\"assistant\", call_specific_model)\n",
                "    g.set_entry_point(\"assistant\")\n",
                "    g.set_finish_point(\"assistant\")\n",
                "    compiled = g.compile()\n",
                "    \n",
                "    result = compiled.invoke({\"messages\": [HumanMessage(content=question)]})\n",
                "    return result[\"messages\"][-1].content\n",
                "\n",
                "# Compare different models\n",
                "question = \"Explain quantum computing in one sentence.\"\n",
                "print(f\"Question: {question}\\n\")\n",
                "\n",
                "# GPT-3.5 Turbo (faster, cheaper)\n",
                "print(\"GPT-3.5 Turbo:\")\n",
                "print(test_model(\"gpt-3.5-turbo\", question))\n",
                "\n",
                "# GPT-4 (more capable, slower)\n",
                "print(\"\\nGPT-4:\")\n",
                "print(test_model(\"gpt-4\", question))\n",
                "\n",
                "# GPT-4 Turbo (faster GPT-4)\n",
                "print(\"\\nGPT-4 Turbo:\")\n",
                "print(test_model(\"gpt-4-turbo\", question))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "notes",
            "metadata": {},
            "source": [
                "## üìù Key Takeaways\n",
                "\n",
                "1. **`ChatOpenAI(model=\"gpt-4\")`** - Initializes the GPT-4 model\n",
                "2. **`llm.invoke(messages)`** - Sends messages and gets a response\n",
                "3. **`add_messages` reducer** - Automatically appends new messages to conversation history\n",
                "4. **System Message** - Sets the assistant's behavior and context\n",
                "5. **State Management** - LangGraph handles conversation state automatically\n",
                "\n",
                "## üîë Requirements\n",
                "\n",
                "Make sure you have a `.env` file with:\n",
                "```\n",
                "OPENAI_API_KEY=sk-your-key-here\n",
                "```\n",
                "\n",
                "## üìö Model Options\n",
                "\n",
                "- `gpt-3.5-turbo` - Fast, cheap, good for simple tasks\n",
                "- `gpt-4` - Most capable, slower, more expensive\n",
                "- `gpt-4-turbo` - Fast GPT-4 with larger context window\n",
                "- `gpt-4o` - Multimodal GPT-4 (text + images)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
